{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Credit Approval\n",
    "## Using dataset from https://archive.ics.uci.edu/ml/datasets/Credit+Approval\n",
    "## Test accuracy of predictions using Random Forest and Logistic Regression\n",
    "## Both perform similarly on this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('credit_approval').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set Schema \n",
    "from pyspark.sql.types import (StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               StructType,\n",
    "                               DoubleType)\n",
    "\n",
    "data_schema = [\n",
    "                StructField('c0', StringType(),True), \n",
    "                StructField('c1', DoubleType(),True),\n",
    "                StructField('c2', DoubleType(),True),\n",
    "                StructField('c3', StringType(),True),\n",
    "                StructField('c4', StringType(),True),\n",
    "                StructField('c5', StringType(),True),\n",
    "                StructField('c6', StringType(),True),\n",
    "                StructField('c7', DoubleType(),True),\n",
    "                StructField('c8', StringType(),True),\n",
    "                StructField('c9', StringType(),True),\n",
    "                StructField('c10', DoubleType(),True),\n",
    "                StructField('c11', StringType(),True),\n",
    "                StructField('c12', StringType(),True),\n",
    "                StructField('c13', DoubleType(),True),\n",
    "                StructField('c14', DoubleType(),True),\n",
    "                StructField('c15', StringType(),True)] \n",
    "\n",
    "final_struct = StructType(fields=data_schema)\n",
    "\n",
    "#Read in data\n",
    "dataset = spark.read.csv('crx.data', header=False,schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c0: string (nullable = true)\n",
      " |-- c1: double (nullable = true)\n",
      " |-- c2: double (nullable = true)\n",
      " |-- c3: string (nullable = true)\n",
      " |-- c4: string (nullable = true)\n",
      " |-- c5: string (nullable = true)\n",
      " |-- c6: string (nullable = true)\n",
      " |-- c7: double (nullable = true)\n",
      " |-- c8: string (nullable = true)\n",
      " |-- c9: string (nullable = true)\n",
      " |-- c10: double (nullable = true)\n",
      " |-- c11: string (nullable = true)\n",
      " |-- c12: string (nullable = true)\n",
      " |-- c13: double (nullable = true)\n",
      " |-- c14: double (nullable = true)\n",
      " |-- c15: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check Schema\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c0',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'c3',\n",
       " 'c4',\n",
       " 'c5',\n",
       " 'c6',\n",
       " 'c7',\n",
       " 'c8',\n",
       " 'c9',\n",
       " 'c10',\n",
       " 'c11',\n",
       " 'c12',\n",
       " 'c13',\n",
       " 'c14',\n",
       " 'c15',\n",
       " 'c8_index',\n",
       " 'c3_index',\n",
       " 'c6_index',\n",
       " 'c12_index',\n",
       " 'c0_index',\n",
       " 'c9_index',\n",
       " 'c5_index',\n",
       " 'c4_index',\n",
       " 'c15_index',\n",
       " 'c11_index',\n",
       " 'c0_index_encoded',\n",
       " 'c3_index_encoded',\n",
       " 'c4_index_encoded',\n",
       " 'c5_index_encoded',\n",
       " 'c6_index_encoded',\n",
       " 'c8_index_encoded',\n",
       " 'c9_index_encoded',\n",
       " 'c11_index_encoded',\n",
       " 'c12_index_encoded']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create indexed, OneHotEncoded categorical values\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(dataset)\n",
    "            for column in \n",
    "            list(set(dataset.columns)-set(['c1', 'c2', 'c7', 'c10', 'c13', 'c14'])) ]\n",
    "\n",
    "indexer_pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "dataset_indexed = indexer_pipeline.fit(dataset).transform(dataset)\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol=column, outputCol=column+'_encoded')\n",
    "           for column in\n",
    "           ['c0_index', 'c3_index', 'c4_index', 'c5_index', 'c6_index', 'c8_index', 'c9_index',\n",
    "            'c11_index', 'c12_index']]\n",
    "\n",
    "encode_pipeline = Pipeline(stages=encoders)\n",
    "\n",
    "dataset_encoded = encode_pipeline.fit(dataset_indexed).transform(dataset_indexed)\n",
    "\n",
    "#Check columns in encoded dataset \n",
    "dataset_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert to features column\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'c0_index_encoded',\n",
    "        'c1',\n",
    "        'c2',\n",
    "        'c3_index_encoded',\n",
    "        'c4_index_encoded',\n",
    "        'c5_index_encoded',\n",
    "        'c6_index_encoded',\n",
    "        'c7',\n",
    "        'c8_index_encoded',\n",
    "        'c9_index_encoded',\n",
    "        'c10',\n",
    "        'c11_index_encoded',\n",
    "        'c12_index_encoded',\n",
    "        'c13',\n",
    "        'c14'    \n",
    "    ], \n",
    "    outputCol='features')\n",
    "\n",
    "compiled_data = assembler.transform(dataset_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get just features and labels columns\n",
    "final_data = compiled_data.select('features', compiled_data['c15_index'].alias('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(37,[0,1,3,5,9,20...|  1.0|\n",
      "|(37,[1,2,3,5,8,21...|  1.0|\n",
      "|(37,[1,2,3,5,8,21...|  1.0|\n",
      "|(37,[0,1,2,3,5,9,...|  1.0|\n",
      "|(37,[0,1,2,3,5,9,...|  1.0|\n",
      "|(37,[0,1,2,3,5,15...|  1.0|\n",
      "|(37,[0,1,2,3,5,21...|  1.0|\n",
      "|(37,[1,2,3,5,14,2...|  1.0|\n",
      "|(37,[0,1,2,4,6,13...|  1.0|\n",
      "|(37,[0,1,2,4,6,9,...|  1.0|\n",
      "|(37,[0,1,2,3,5,7,...|  1.0|\n",
      "|(37,[0,1,2,3,5,7,...|  1.0|\n",
      "|(37,[1,2,3,5,13,2...|  1.0|\n",
      "|(37,[0,1,2,3,5,13...|  1.0|\n",
      "|(37,[1,2,3,5,8,20...|  1.0|\n",
      "|(37,[0,1,2,4,6,13...|  1.0|\n",
      "|(37,[0,1,2,3,5,15...|  1.0|\n",
      "|(37,[1,2,3,5,8,20...|  1.0|\n",
      "|(37,[0,1,2,3,5,17...|  1.0|\n",
      "|(37,[1,2,3,5,14,2...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Train and Test sets\n",
    "train, test = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classification\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(numTrees=100)\n",
    "rfc_model = rfc.fit(train)\n",
    "rfc_pred = rfc_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "0.888235294117647\n"
     ]
    }
   ],
   "source": [
    "#Evaluate accuracy of RF classification\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "print('RFC Accuracy')\n",
    "print(acc_eval.evaluate(rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logistic Regression Classification\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr_model = lr.fit(train)\n",
    "lr_predict = lr_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy\n",
      "0.9240701754385956\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Logistic Regression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "eval_bin = BinaryClassificationEvaluator()\n",
    "print('Logistic Regression Accuracy')\n",
    "print(eval_bin.evaluate(lr_predict.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
